{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "###########################################\n",
    "#               FUNCTIONS                 #\n",
    "###########################################\n",
    "\n",
    "\n",
    "def clean_reference(ref,outliers):\n",
    "    for i in outliers:\n",
    "        ref = ref.drop(labels=i,axis=1)\n",
    "\n",
    "    return ref\n",
    "\n",
    "def norm_ref(ref):\n",
    "    med = ref.median(axis=0)\n",
    "    norm_ref = ref/med\n",
    "    return norm_ref, med\n",
    "\n",
    "def create_synthetic(norm_ref,med,N):\n",
    "    synt = pd.DataFrame(index=norm_ref.index,columns=range(N))\n",
    "    for j in range(N):\n",
    "        for i in norm_ref.index:\n",
    "            synt[j][i] = np.random.choice(norm_ref.loc[norm_ref.index==i].values.flatten())\n",
    "\n",
    "        synt[j] = synt[j] * np.random.choice(med)\n",
    "    synt.columns = [\"sample_\" + str(i) for i in range(synt.shape[1])]\n",
    "    return synt\n",
    "\n",
    "def add_features(synt,sample,gene,factor,exon=None):\n",
    "    if exon is None:\n",
    "        pattern = gene\n",
    "        tmp = [str(synt.index[i]).split(\"_\")[0] for i in range(synt.shape[0])]\n",
    "        synt.loc[[tmp[i]==pattern for i in range(len(tmp))],sample] = synt.loc[[tmp[i]==pattern for i in range(len(tmp))]][sample] * factor\n",
    "        \n",
    "    if exon is not None:\n",
    "        pattern = gene + \"_\" + exon\n",
    "        tmp = [str(synt.index[i]).split(\"_\")[0] + \"_\" + str(synt.index[i]).split(\"_\")[1] for i in range(synt.shape[0])] \n",
    "        synt.loc[[tmp[i]==pattern for i in range(len(tmp))],sample] = synt.loc[[tmp[i]==pattern for i in range(len(tmp))]][sample] * factor\n",
    "    \n",
    "    res = synt\n",
    "    return res\n",
    "\n",
    "\n",
    "def openJson(path,n):\n",
    "    \"\"\"\n",
    "    Opens json files in path to create a reads matrix\n",
    "    \"\"\"\n",
    "    tmp = os.listdir(path)\n",
    "    tmp = np.array(tmp)[np.array([bool(re.findall(\"depths.json$\",tmp[i])) for i in range(len(tmp))])]\n",
    "    reads = np.zeros((n,len(tmp)))\n",
    "    amplicons = [\"\" for x in range(n)]\n",
    "    q=0\n",
    "    for p in tmp:\n",
    "        with open(path+p) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for i in range(n):\n",
    "                amplicons[i] = data[i]['name']\n",
    "                for j in data[i]['depths']:\n",
    "                    reads[i,q] = int(data[i]['depths'][j]['min'])\n",
    "        q=q+1\n",
    "    reads = pd.DataFrame(data = reads,index=amplicons)\n",
    "    reads.columns = [i.split('_')[0]+'_'+i.split('_')[1] for i in tmp]\n",
    "    return reads\n",
    "\n",
    "def sumLibraries(reads):\n",
    "    samples = np.unique([i.split('_')[0] for i in reads.columns])\n",
    "    reads_f = np.zeros((reads.shape[0],len(samples)))\n",
    "    q=0\n",
    "    for i in samples:\n",
    "        sub = reads.filter(regex=\"^\"+i)\n",
    "        reads_f[:,q] = sub.sum(axis=1)\n",
    "        q=q+1\n",
    "    reads_f = pd.DataFrame(data = reads_f,index=reads.index)\n",
    "    reads_f.columns = list(samples)\n",
    "    return(reads_f)\n",
    "\n",
    "def correctIndex(reads,correspondance):\n",
    "    l = [\"\" for x in range(len(reads.index))]\n",
    "    q=0\n",
    "    for i in reads.index:\n",
    "        l[q] = i[(len(i)-9):len(i)]\n",
    "        q=q+1\n",
    "    final = reads[[correspondance[\"amplicon\"][0] in l[x] for x in range(len(l))]]\n",
    "    for i in correspondance[\"amplicon\"]:\n",
    "        if i!=correspondance[\"amplicon\"][0]:\n",
    "            final = pd.concat([final,reads[[i in l[x] for x in range(len(l))]]])\n",
    "    final.index = correspondance[\"gene_exon\"] + \"_\" + correspondance[\"amplicon\"]\n",
    "    return final\n",
    "\n",
    "\n",
    "def filterReads(reads,N,output_path):\n",
    "    reads = reads.loc[:,reads.sum(axis=0)>N]\n",
    "    reads = reads.filter(regex=\"^(?!MSI)\",axis=0)\n",
    "    reads = reads.filter(regex=\"^(?!TN)\")\n",
    "    reads = reads.filter(regex=\"^(?!TP)\")\n",
    "    reads = reads.filter(regex=\"^(?!HD)\")\n",
    "    reads = reads.filter(regex=\"^(?!H2)\")\n",
    "    reads.to_csv(output_path, sep=\"\\t\",index=True)\n",
    "    return(reads)\n",
    "\n",
    "\n",
    "def normalizeReads(reads,output_path=None,save=False):\n",
    "    reads_norm=reads/reads.median(axis=0)\n",
    "    reads = np.log(reads+1)\n",
    "    if save==True:\n",
    "        reads_norm.to_csv(output_path, sep=\"\\t\",index=True)\n",
    "    return(reads_norm)\n",
    "\n",
    "\n",
    "def aberrantSamples(reads,conta='auto'):\n",
    "    reads = reads/np.sum(reads)\n",
    "    \n",
    "    tmp = np.percentile(reads, 99, axis = 0)/np.mean(reads, axis = 0)\n",
    "    random_data = np.array(tmp).reshape(-1,1)\n",
    "    clf = IsolationForest(contamination=conta).fit(random_data)\n",
    "    preds = clf.predict(random_data)\n",
    "    res_amp = np.array(reads.columns)[preds==-1]\n",
    "    \n",
    "    tmp = np.percentile(reads, 1, axis = 0)/np.mean(reads, axis = 0)\n",
    "    random_data = np.array(tmp).reshape(-1,1)\n",
    "    clf = IsolationForest(contamination=conta).fit(random_data)\n",
    "    preds = clf.predict(random_data)\n",
    "    res_del = np.array(reads.columns)[preds==-1]\n",
    "    \n",
    "    res = np.unique(np.concatenate((res_amp,res_del)))\n",
    "    \n",
    "    return(res)\n",
    "\n",
    "\n",
    "def aberrantSamples2(reads):\n",
    "    read = reads.astype(\"float\")\n",
    "    tmp = np.percentile(reads, 5, axis = 0)/np.mean(reads, axis = 0)\n",
    "    random_data = np.array(tmp).reshape(-1,1)\n",
    "\n",
    "    clf = IsolationForest(contamination=0.1).fit(random_data)\n",
    "    preds = clf.predict(random_data)\n",
    "    res = np.array(reads.columns)[preds==-1]\n",
    "    return(res)\n",
    "\n",
    "\n",
    "def aberrantAmplicons(reads_norm,abSamples):\n",
    "    for name in res:\n",
    "        random_data = np.array(reads_norm[name]).reshape(-1,1)\n",
    "        clf = IsolationForest(contamination=0.001).fit(np.array(np.mean(reads_norm, axis = 1)).reshape(-1,1))\n",
    "        preds = clf.predict(random_data)\n",
    "        print(name)\n",
    "        print(np.array(reads_norm.index)[preds==-1])\n",
    "\n",
    "def aberrantAmpliconsPerSample(name,reads_norm,conta='auto',verbose=False):\n",
    "    random_data = np.array(reads_norm[name]).reshape(-1,1)\n",
    "    clf = IsolationForest(contamination=conta).fit(np.array(np.mean(reads_norm, axis = 1)).reshape(-1,1))\n",
    "    preds = clf.predict(random_data)\n",
    "    if verbose:\n",
    "        print(name)\n",
    "        print(np.array(reads_norm.index)[preds==-1])\n",
    "    return(np.array(reads_norm.index)[preds==-1])\n",
    "\n",
    "\n",
    "def aberrantAmpliconsPerSample2(name,reads,abSamples,verbose=False):\n",
    "    ab = [i in abSamples for i in reads.columns]\n",
    "    normalReads = reads[np.delete(reads.columns,ab)]\n",
    "    med = np.percentile(normalReads, 99, axis = 1)\n",
    "    reads = (reads.T/med).T\n",
    "    random_data = np.array(reads[name]).reshape(-1,1)\n",
    "    clf = IsolationForest(contamination=0.05).fit(np.array(np.median(reads, axis = 1)).reshape(-1,1))\n",
    "    preds = clf.predict(random_data)\n",
    "    if verbose:\n",
    "        print(name)\n",
    "        print(np.array(reads.index)[preds==-1])\n",
    "    return(np.array(reads.index)[preds==-1])\n",
    "\n",
    "\n",
    "\n",
    "def percentagePerExon(amplified,reads,verbose=False):\n",
    "    genes = [i.split('_')[0] for i in reads.index]\n",
    "    exons = [i.split('_')[1] for i in reads.index]\n",
    "    g_e = [genes[i]+'_'+exons[i] for i in range(len(genes))]\n",
    "    n_ge = np.array([g_e.count(i) for i in np.unique(g_e)])\n",
    "    ag = [i.split('_')[0] for i in amplified]\n",
    "    ae = [i.split('_')[1] for i in amplified]\n",
    "    age = [ag[i]+'_'+ae[i] for i in range(len(amplified))]\n",
    "    f = pd.DataFrame(index=np.unique(age),columns=[\"percentage\"])\n",
    "    f = f.fillna(0)\n",
    "    for i in range(len(np.unique(age))):\n",
    "        f['percentage'][i] = 100*float(age.count(''.join(np.unique(age)[i]))/n_ge[np.unique(g_e)==''.join(np.unique(age)[i])])\n",
    "        if verbose:\n",
    "            if f['percentage'][i]>50:\n",
    "                print(np.unique(age)[i] + \": \" + str(round(f['percentage'][i]))+'%'+' des amplicons de l\\'exon sont aberrants')\n",
    "    return(f)\n",
    "\n",
    "def percentagePerGene(amplified,reads,verbose=False):\n",
    "    genes = [i.split('_')[0] for i in reads.index]\n",
    "    ag = [i.split('_')[0] for i in amplified]\n",
    "    n_g = np.array([genes.count(i) for i in np.unique(genes)])\n",
    "    f = pd.DataFrame(index=np.unique(ag),columns=[\"percentage\"])\n",
    "    f = f.fillna(0)\n",
    "    for i in range(len(np.unique(ag))):\n",
    "        f['percentage'][i] = 100*float(ag.count(''.join(np.unique(ag)[i]))/n_g[np.unique(genes)==''.join(np.unique(ag)[i])])\n",
    "        if verbose:\n",
    "            if f['percentage'][i]>50:\n",
    "                print(np.unique(ag)[i] + \": \" + str(round(f['percentage'][i]))+'%'+' des amplicons du gene sont aberrants')\n",
    "    return(f)\n",
    "\n",
    "def amplifEvalGene(reads,abSamples,gene,sample):\n",
    "    reads_m = reads/reads.median(axis=0)\n",
    "    sub = reads_m\n",
    "    for i in abSamples:\n",
    "        sub = sub.drop(labels=i,axis=1)\n",
    "    reads_m = reads_m.filter(regex=\"^\"+gene,axis=0)\n",
    "    reads_m = reads_m[sample]   \n",
    "    val = np.mean(reads_m)/np.mean(sub.mean())\n",
    "    if val==np.inf:\n",
    "        val = 100\n",
    "    return val\n",
    "\n",
    "def scoreAmplif(k,n,N):\n",
    "    p = n/N\n",
    "    x = np.log(1/((p**k)*(1-p)**(n-k)))*(k/n)\n",
    "    # score = 1/(1+np.exp(-x))\n",
    "    score = x/390 + 190/390\n",
    "    \n",
    "    return x\n",
    "\n",
    "def aberrantAmpliconsFinal(reads, reads_norm, abSamples,abSamples2,run,threshold):\n",
    "    f = pd.DataFrame(columns=[\"run\",\"name\",\"gene\",\"amplif\",\"score\"])\n",
    "    q=0 \n",
    "    for name in abSamples2:\n",
    "        #abAmp = aberrantAmpliconsPerSample2(name,reads_norm,abSamples,verbose=False)\n",
    "        abAmp = aberrantAmpliconsPerSample(name,reads_norm,verbose=False)\n",
    "        if abAmp.shape!=(0,):\n",
    "            genes = np.unique([i.split('_')[0] for i in abAmp])\n",
    "            for gene in genes:\n",
    "                r = re.compile(gene)\n",
    "                abEx = list(filter(r.match, abAmp))\n",
    "                exons1 = [i.split('_')[0]+\"_\"+i.split('_')[1] for i in abEx]\n",
    "                tmp = reads.filter(regex=\"^\"+gene,axis=0)\n",
    "                exons2 = [i.split('_')[0]+\"_\"+i.split('_')[1] for i in tmp.index]\n",
    "                \n",
    "                score = scoreAmplif(len(abEx),tmp.shape[0],reads.shape[0])\n",
    "                \n",
    "                amplif = amplifEvalGene(reads, abSamples, gene, name)\n",
    "\n",
    "                if score>threshold:\n",
    "                    if amplif>1:\n",
    "                        f.loc[q] = [run,name,gene,amplif,score]\n",
    "                        q=q+1\n",
    "                    #if amplif<1:\n",
    "                    #    f.loc[q] = [run,name,gene,amplif,score]\n",
    "                    #    q=q+1\n",
    "\n",
    "    return(f)\n",
    "\n",
    "\n",
    "def aberrantAmpliconsFinal2(reads, reads_norm, abSamples,abSamples2,run,threshold):\n",
    "    f = pd.DataFrame(columns=[\"run\",\"name\",\"gene\",\"amplif\",\"score\"])\n",
    "        \n",
    "    q=0 \n",
    "    for name in abSamples2:\n",
    "        #abAmp = aberrantAmpliconsPerSample2(name,reads_norm,abSamples,verbose=False)\n",
    "        abAmp = aberrantAmpliconsPerSample(name,reads_norm,verbose=False)\n",
    "        if abAmp.shape!=(0,):\n",
    "            genes = abAmp\n",
    "            for gene in genes:\n",
    "                r = re.compile(gene)\n",
    "                abEx = list(filter(r.match, abAmp))\n",
    "                #print(abEx)\n",
    "                tmp = reads.filter(regex=\"^\"+gene,axis=0)                \n",
    "                score = scoreAmplif(len(abEx),tmp.shape[0],reads.shape[0])\n",
    "                \n",
    "                amplif = amplifEvalGene(reads, abSamples, gene, name)\n",
    "\n",
    "                if score>threshold:\n",
    "                    if amplif>1:\n",
    "                        f.loc[q] = [run,name,gene,amplif,score]\n",
    "                        q=q+1\n",
    "                    #if amplif<1:\n",
    "                    #    f.loc[q] = [run,name,gene,amplif,score]\n",
    "                    #    q=q+1\n",
    "\n",
    "    return(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdd5ab03f5c486091ec726bbab8fd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "\n",
    "def aberrationDetection(reads,conta='auto'):\n",
    "    reads_norm = reads/np.median(reads)\n",
    "    reads_norm = reads_norm.T/np.sum(reads_norm.T)\n",
    "    reads_norm = reads_norm.T\n",
    "    final = pd.DataFrame(0,index=reads.index,columns=reads.columns)\n",
    "    for i in reads.index:\n",
    "        tmp = reads_norm.loc[i]\n",
    "        random_data = np.array(tmp).reshape(-1,1)\n",
    "        clf = IsolationForest(contamination=conta).fit(random_data)\n",
    "        preds = clf.predict(random_data)\n",
    "        final.loc[i] = 0.5*(1-preds)\n",
    "\n",
    "    return(final)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/Users/admin/Documents/CNV/article/test_dataset_capture_3.txt\",sep=\"\\t\",index_col=0)\n",
    "run = \"test_capture\"\n",
    "output_path = \"/Users/admin/Documents/CNV/article/binary/\"\n",
    "normal = pd.read_csv(\"/Users/admin/Documents/CNV/article/capture_normal_samples.tsv\",sep=\"\\t\",index_col=None)\n",
    "normal = normal[\"neg\"]\n",
    "normal = [str(i) for i in normal]\n",
    "\n",
    "#anormal = pd.read_csv(\"/Users/admin/Documents/CNV/article/multi_cnv_samples.txt\",sep=\"\\t\",index_col=None)\n",
    "#anormal = anormal[\"samples\"]\n",
    "anormal = pd.read_csv(\"/Users/admin/Documents/CNV/article/capture_anormal_samples.tsv\",sep=\"\\t\",index_col=None)\n",
    "anormal = anormal[\"ref\"]\n",
    "anormal = [str(i) for i in anormal]\n",
    "\n",
    "for k in tqdm(range(50)):\n",
    "    norms = random.sample(normal,19)\n",
    "    anorms = random.sample(anormal,1)\n",
    "    select = norms+anorms\n",
    "    sub = data[select]\n",
    "\n",
    "    res = aberrationDetection(sub,conta=0.01)\n",
    "    res.to_csv(output_path+\"binary_matrix_all_\"+str(k+1)+\".txt\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98c869fb858438295a226671f39deea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "def aberrationDetection(reads,conta='auto'):\n",
    "    reads_norm = reads/np.median(reads)\n",
    "    reads_norm = reads_norm.T/np.sum(reads_norm.T)\n",
    "    reads_norm = reads_norm.T\n",
    "    final = pd.DataFrame(0,index=reads.index,columns=reads.columns)\n",
    "    for i in reads.index:\n",
    "        tmp = reads_norm.loc[i]\n",
    "        random_data = np.array(tmp).reshape(-1,1)\n",
    "        clf = IsolationForest(contamination=conta).fit(random_data)\n",
    "        preds = clf.predict(random_data)\n",
    "        final.loc[i] = 0.5*(1-preds)\n",
    "\n",
    "    return(final)\n",
    "\n",
    "\n",
    "def aberrantAmpliconsPerSample(name,reads_norm,conta='auto',verbose=False):\n",
    "    random_data = np.array(reads_norm[name]).reshape(-1,1)\n",
    "    clf = IsolationForest(contamination=conta).fit(np.array(np.mean(reads_norm, axis = 1)).reshape(-1,1))\n",
    "    preds = clf.predict(random_data)\n",
    "    if verbose:\n",
    "        print(name)\n",
    "        print(np.array(reads_norm.index)[preds==-1])\n",
    "    return(np.array(reads_norm.index)[preds==-1])\n",
    "\n",
    "def aberrantAmpliconsPerSample(name,reads_norm,conta='auto',verbose=False):\n",
    "    random_data = np.array(reads_norm[name]).reshape(-1,1)\n",
    "    clf = IsolationForest(contamination=conta).fit(np.array(np.mean(reads_norm, axis = 1)).reshape(-1,1))\n",
    "    preds = clf.predict(random_data)\n",
    "    if verbose:\n",
    "        print(name)\n",
    "        print(np.array(reads_norm.index)[preds==-1])\n",
    "    return(np.array(reads_norm.index)[preds==-1])\n",
    "\n",
    "\n",
    "def amplifEval(reads,abSamples,abAmp,sample):\n",
    "    reads_m = reads/reads.median(axis=0)\n",
    "    sub = reads_m\n",
    "    for i in abSamples:\n",
    "        sub = sub.drop(labels=i,axis=1)\n",
    "    reads_m = reads_m.filter(regex=\"^\"+abAmp,axis=0)\n",
    "    reads_m = reads_m[sample]\n",
    "    val = reads_m/np.mean(sub.mean())\n",
    "    if np.mean(sub.mean())==0:\n",
    "        val = 100\n",
    "    return float(val)\n",
    "\n",
    "data = pd.read_csv(\"/Users/admin/Documents/CNV/article/test_dataset_capture_3.txt\",sep=\"\\t\",index_col=0)\n",
    "run = \"test_capture\"\n",
    "output_path = \"/Users/admin/Documents/CNV/article/capture_res/\"\n",
    "data_norm = normalizeReads(data,output_path)\n",
    "\n",
    "normal = pd.read_csv(\"/Users/admin/Documents/CNV/article/capture_normal_samples.tsv\",sep=\"\\t\",index_col=None)\n",
    "normal = normal[\"neg\"]\n",
    "normal = [str(i) for i in normal]\n",
    "\n",
    "anormal = pd.read_csv(\"/Users/admin/Documents/CNV/article/capture_anormal_samples.tsv\",sep=\"\\t\",index_col=None)\n",
    "anormal = anormal[\"ref\"]\n",
    "#anormal = pd.read_csv(\"/Users/admin/Documents/CNV/article/multi_cnv_samples.txt\",sep=\"\\t\",index_col=None)\n",
    "#anormal = anormal[\"samples\"]\n",
    "anormal = [str(i) for i in anormal]\n",
    "\n",
    "\n",
    "for k in tqdm(range(20)):\n",
    "    norms = random.sample(normal,17)\n",
    "    anorms = random.sample(anormal,3)\n",
    "    select = norms+anorms\n",
    "    sub = data[select]\n",
    "    sub_norm = normalizeReads(sub)\n",
    "    res = aberrationDetection(sub,conta=0.01)\n",
    "    det = res.columns[np.sum(res)==0]\n",
    "    normal_data = sub[det]/np.median(sub[det])\n",
    "    data2 = data_norm.T/np.mean(normal_data.T)\n",
    "    data2.T.to_csv(output_path+\"data_capture_sur_normal_\"+str(k+1)+\".tsv\", sep=\"\\t\",index=True)\n",
    "    #f = pd.DataFrame(columns=[\"run\",\"name\",\"target\",\"amplif\"])\n",
    "    #q=0\n",
    "    #for name in det:\n",
    "    #    abAmp = aberrantAmpliconsPerSample(str(name),sub_norm)\n",
    "    #    for i in abAmp:\n",
    "    #        amplif = amplifEval(sub,det,i,str(name))\n",
    "    #        f.loc[q] = [run,name,i,amplif]\n",
    "    #        q=q+1\n",
    "\n",
    "    #f.to_csv(output_path+\"res_capture_neg_auto_3samples_\"+str(k+1)+\".tsv\", sep=\"\\t\",index=True)\n",
    "    pd.DataFrame(norms).to_csv(output_path+\"normal_csn_samples_\"+str(k+1)+\".tsv\", sep=\"\\t\",index=True)\n",
    "    pd.DataFrame(anorms).to_csv(output_path+\"anormal_csn_samples_\"+str(k+1)+\".tsv\", sep=\"\\t\",index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.in1d(sub.columns,det)\n",
    "\n",
    "normal_data = sub[det]/np.median(sub[det])\n",
    "sum(np.mean(normal_data.T)==0)\n",
    "data2 = data_norm.T/np.mean(normal_data.T)\n",
    "data2.T.to_csv(output_path+\"data_capture_sur_normal.tsv\", sep=\"\\t\",index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
